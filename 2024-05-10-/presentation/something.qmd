---
title: |
  <span style="font-size:.8em;">Net-positive <br> Research Software Gardening</span>
format:
  revealjs: 
    theme: simple
    transition: slide
    slide-number: true
    auto-stretch: false
    width: 1050
    height: 800
    margin: 0.01
---

## Gratitude

<br><br><br><br>

___Thank you___ for listening, asking questions,<br> and helping make this conversation better!

------------------------------------------------------------------------

## Outline

<br>
<br>
<br>

1. üü§ Mud
1. ‚ö° Energy
1. üëÅÔ∏è Vision

------------------------------------------------------------------------

## We need to talk about mud

<img src="assets/Ambro≈ºy_Sabatowski_Mud_in_the_forest_1922.jpeg" width="800px;"><br>
Mud can be friend or foe.<br>
<span style="font-size:.6em;font-style:italic;color:#555;">Image: Ambro≈ºy Sabatowski, Mud in the forest, 1922 (<a href="https://commons.wikimedia.org/wiki/File:Ambro%C5%BCy_Sabatowski_Mud_in_the_forest_1922.jpg">Wikimedia Commons</a>)</span>

------------------------------------------------------------------------

## Mud as friend

<div style="float:left;width:50%;">
<ul>
<li>Adobe (Spanish for _mudbrick_) is an over 7,000 year old technology ([Wikipedia: Adobe](https://en.wikipedia.org/wiki/Adobe)).</li>
<li>Innovations like [Superadobe](https://en.wikipedia.org/wiki/Superadobe) continue to enhance the use of mud.</li>
<li>_Mud Futures_ demos 3D printed mud structures ([History Colorado Center, Denver](https://www.historycolorado.org/exhibit/mud-futures))</li>
</ul>
</div>

<img src="assets/Flickr_-_DVIDSHUB_-_New_eco-dome_signals_changes_for_local_village_(Image_10_of_10).jpeg" height="600px;" style="float:right;">


<div style="float:left;width:100%;">
<span style="font-size:.6em;font-style:italic;color:#555;">Image: DVIDSHUB, 2012 (<a href="https://commons.wikimedia.org/wiki/File:Flickr_-_DVIDSHUB_-_New_eco-dome_signals_changes_for_local_village_(Image_10_of_10).jpg">Wikimedia Commons</a>)</span>
</div>

------------------------------------------------------------------------

## Mud as foe

<img src="assets/CA-84_Woodside_April_2023_002.jpeg" width="800px;"><br>
Mud can also cause damage if left untended.<br>
<span style="font-size:.6em;font-style:italic;color:#555;">Image: Ambro≈ºy 	King of Hearts, 2023 (<a href="https://commons.wikimedia.org/wiki/File:CA-84_Woodside_April_2023_002.jpg">Wikimedia Commons</a>)</span>

------------------------------------------------------------------------

## Software mud

![](assets/2024-05-07-12-30-52.png)

<a href="http://www.laputan.org/mud/mud.html">Foote, Brian, and Joseph Yoder. "Big ball of mud." _Pattern languages of program design 4_ (1997)</a>

<br>

_"A BIG BALL OF MUD is haphazardly structured, sprawling, sloppy, duct-tape and bailing wire, spaghetti code jungle."_

------------------------------------------------------------------------

## Software mud as necessary

<br><br>

![](assets/2024-05-07-13-34-14.png)

Software projects often move from ___"throwaway code"___ (little balls of mud) to ___"piecemeal growth"___ and ___"keep it working"___ which can emerge as ___"big balls of mud"___ along the journey.

------------------------------------------------------------------------

## Defining a space for challenges

<br>

![](assets/2024-05-07-14-51-55.png)

Sometimes we need to ___"sweep it under the rug"___.

_"Overgrown, tangled, haphazard spaghetti code is hard to comprehend, repair, or extend, and tends to grow even worse if it is not somehow brought under control."_

------------------------------------------------------------------------

## Software design as continuous

<br>

> _"Things that are good have a certain kind of structure. You can‚Äôt get that structure except dynamically. Period. __In nature you‚Äôve got continuous very-small-feedback-loop adaptation going on, which is why things get to be harmonious__. That‚Äôs why they have the qualities we value. If it wasn‚Äôt for the time dimension, it wouldn‚Äôt happen."_
>
> <br>
> \- Christopher Alexander (Brand, Stewart. _How buildings learn: What happens after they're built._ 1995)

------------------------------------------------------------------------

## Movement in mud


<img src="assets/Moving_a_house_with_a_team_of_horses_through_the_mud_down_Front_St,_Nome,_Alaska,_August_27,_1900_(HEGG_468).jpeg" width="700px;"><br>
Movement through mud takes energy!<br>
<span style="font-size:.6em;font-style:italic;color:#555;">Image: Eric A. Hegg, 1900 (<a href="https://commons.wikimedia.org/wiki/File:Moving_a_house_with_a_team_of_horses_through_the_mud_down_Front_St,_Nome,_Alaska,_August_27,_1900_(HEGG_468).jpeg">Wikimedia Commons</a>)</span>


------------------------------------------------------------------------

## Knowledge work

![](assets/2024-05-07-15-59-25.png)

- Software work is [knowledge work](https://en.wikipedia.org/wiki/Knowledge_worker).
- Knowledge work is afforded through a complex network of energy sources.

------------------------------------------------------------------------

## Software knowledge work fatigue

<br>

> "66% of developers rated the severity of fatigue during programming tasks as high or very high. 59% of developers rated the frequency of their fatigue during programming tasks as often or very often. Stress and sleepiness were the most voted causes."

\- S. Sarkar and C. Parnin, "Characterizing and Predicting Mental Fatigue during Programming Tasks," 2017 ([link](ttps://ieeexplore.ieee.org/abstract/document/7961890))

------------------------------------------------------------------------

## Software fatigue - an estimate

<br><br>

How much mud can we cultivate (ideally)?

- 6 hours a day to work (minus chats, restroom, food, travel)
- 3 hours a day for ["deep work"](https://en.wikipedia.org/wiki/Cal_Newport#Attention_management) (minus interruptions)
- 5 days a week * 3 hours = 15 hours a week for deep software work

------------------------------------------------------------------------

## Software fatigue and piecemeal growth



We're up against time on two fronts:

- Piecemeal growth requires incremental change.
- Development can only take place through a small window.

It's crucial to be selective about what software development we do.

------------------------------------------------------------------------

## Data Challenges

<center>
<img src="https://github.com/zarr-developers/zarr-illustrations-falk-2022/raw/1.0.2/300dpi/kits-deluge-300dpi.png" style="width:300px">
</center>


Large data handling often involve working with data values we cannot see (especially from [big data V's](https://en.wikipedia.org/wiki/Big_data#Characteristics): volume, variety, and velocity).

<br>

<span style="font-size:.7em;font-style:italic;">(Image: ["Kit's Deluge" by Henning Falk, ¬©2022 NumFOCUS, CC BY 4.0.](https://zenodo.org/records/7037679))</span>

------------------------------------------------------------------------

## Data Challenges

<br><br>

> "Some things have to be believed to be seen."

- [Madeleine L'Engle, A Wrinkle in Time (1962)](https://www.goodreads.com/quotes/59274-some-things-have-to-be-believed-to-be-seen)

<br>

Believe with me for a moment in the promise of Parquet and the data that we cannot directly see.

------------------------------------------------------------------------

## Data Challenges

<br>

Besides believing, there are many measured performance benefits to using formats like Parquet (typically orders of magnitude).

<br>

See: 

- CSV vs Parquet - [Speed up data analytics and wrangling with Parquet files (Posit)](https://posit.co/blog/speed-up-data-analytics-with-parquet-files/)
- CSV vs Parquet - [Apache Parquet vs. CSV Files (DZone)](https://dzone.com/articles/how-to-be-a-hero-with-powerful-parquet-google-and)

------------------------------------------------------------------------

## Parquet - Definition

<br><br>

_"Apache Parquet is a columnar storage format available to any project in the Hadoop ecosystem, regardless of the choice of data processing framework, data model or programming language."_

- [Apache Parquet Website](https://parquet.apache.org/)

------------------------------------------------------------------------

## Parquet - History

<br><br>

Some rough history:

- Inspired by earlier work from Google Research in 2010: [Dremel: Interactive Analysis of Web-Scale Datasets](https://research.google/pubs/dremel-interactive-analysis-of-web-scale-datasets-2/).
- Originally introduced by Twitter in collaboration with Cloudera in July 2013 ([link](https://blog.twitter.com/engineering/en_us/a/2013/announcing-parquet-10-columnar-storage-for-hadoop)).
- Parquet joined the Apache Software Foundation in 2015 as a Top-Level Project (TLP) ([link](https://news.apache.org/foundation/entry/the_apache_software_foundation_announces75))

------------------------------------------------------------------------

## Parquet - As a file

<br><br>

```text
file.parquet (unable to preview with a text editor)
```

<br>

Parquet file of the table we saw earlier.

------------------------------------------------------------------------

## Parquet - "Columnar data"

<br>


![](assets/2024-03-28-10-12-29.png)

What does ___"columnar data"___ mean?


------------------------------------------------------------------------

## Parquet - Internals

<br><br>

<!-- mermaid source:

```mermaid
flowchart TB
subgraph parquet_file["Parquet File"]
direction TB
subgraph rowgroup_1 ["Row group 1"]
    direction TB
    subgraph column_1_a["Column A"]
        direction TB
        page_1_a_0["Page 0"]
        page_1_a_1["Page 1"]
    end
    subgraph column_1_b["Column B"]
        direction TB
        page_1_b_0["Page 0"]
        page_1_b_1["Page 1"]
    end
end

subgraph rowgroup_2 ["Row group 2"]
    direction TB
    subgraph column_2_a["Column A"]
        direction TB
        page_2_a_0["Page 0"]
        page_2_a_1["Page 1"]
    end
    subgraph column_2_b["Column B"]
        direction TB
        page_2_b_0["Page 0"]
        page_2_b_1["Page 1"]
    end

end
end
```
-->
![](https://mermaid.ink/img/pako:eNqdkl1PgzAUhv9Kc67JQguswJ3z49qoV4ohhXaMCC12JXMu--8WdEySzbk1pOGcnoe8POkGcsUFxDCv1CpfMG3Q0yyRyzYrNGsWqGH6vRUmnZeVeEng_rtEd7ZM4DWRvNQiN6WSY0yrVaFV26QYWepBrVBfItxDyK4x2HUGOFdVW8sUp8yy132BrgbwMNythhWio1K3T1oI5I6o4RzvzvdxhOTHUmT7FLP_p8hOpMiOpui3Qy7J2CU5wyW5yCU54ZKc6ZJc5JKccEn-cPlj027gQC10zUpuL_umG0nALERtr3FsXznTbwkkcmvnWGvU41rmEBvdCgfahjMjbkpmf6aGeM6q5dC95aVRejfZMPmsVP2rhHgDHxBj15sQ6k6pj6Mg8F0aObC27RBPiB-F9gnCkFJCtw589l9wJ1MXezSIPOpPcRh42y9E-SLo?type=png)

<br>

Parquet organizes column values into __pages__ inside of __row groups__ ([link](https://parquet.apache.org/docs/file-format/)).

------------------------------------------------------------------------

## Parquet - Notable Features

<br><br><br><br>

"Okay already, show me why any of this matters!"

<br><br>

(___How can I use this today?!___)

------------------------------------------------------------------------

## Parquet - "Strongly-typed" Data

<br><br>

```html
Col_A,Col B,Col_C,COL_D
,a,"0.01"
2,null,0.02,{'color':'blue'}
```

<br>

Have you ever experienced mixed data types in your data?

<br>

These are challenging (and expensive) to resolve!

------------------------------------------------------------------------

## Parquet - "Strongly-typed" Data

```python
import pyarrow as pa
from pyarrow import parquet

# create a pyarrow table
table = pa.Table.from_pydict(
    {
        "A": [1, 2, 3],
        "B": ["foo", "bar", 1],
        "C": [0.1, 0.2, 0.3],
    }
)

# write the pyarrow table to a parquet file
parquet.write_table(table=table, where="example.parquet")

# raises exception:
# ArrowTypeError: Expected bytes, got a 'int' object (for column B)
# Note: while this is an Arrow in-memory data exception, it also
# prevents us from attempting to perform incompatible operations
# within the Parquet file.
```

Data within Parquet is considered ["strongly-typed"](https://en.wikipedia.org/wiki/Strong_and_weak_typing), entailing specific data types (such as integer, string, etc.) associated with each column and value.

------------------------------------------------------------------------

## Parquet - Complex data handling

<br>

![](https://mermaid.ink/img/pako:eNpVkL1OwzAUhV_FuhNIaZV_JxmYYGNAsIEZLo3TWtR2cBxBibrBxojECOIJ-whYcaha6Q73nO_oHtkDLHTNoQKmmrV-XqzQWHJ5zZTq5QM33R2D3ffnL5kkg3umOmuEWk7oh0xyRELiknvy8U68GgEagxsPvt6IVyNo0Tz13Dpy5TfSiDU_6U5HOtWS2eyMTMl9_7Hpu449X3PoQQCSG4midk8emCKEgV1xyRlUbq3RPDL3FVuXw97qm41aQGVNzwPo2xotPxe4NCihanDd7d2LWlht_pMtqlut5YGEaoAXqKIwmcc0zGkalVmWhrQMYOPsIprHaVm4yYqC0phuA3gdL4TzPIwSmpUJTfOoyJLtH3SGlcA?type=png)

<br>

Parquet files may store many data types that are complicated or impossible to store in other formats.

------------------------------------------------------------------------

## Parquet - Complex data handling

```python
import pyarrow as pa
from pyarrow import parquet

# create a pyarrow table with complex data types
table = pa.Table.from_pydict(
    {
        "A": [{"key1": "val1"}, {"key2": "val2"}],
        "B": [[1, 2], [3, 4]],
        "C": [
            bytearray("üòä".encode("utf-8")),
            bytearray("üåª".encode("utf-8")),
        ],
    }
)

# write the pyarrow table to a parquet file
parquet.write_table(table=table, where="example.parquet")

# read the schema of the parquet file
print(parquet.read_schema(where="example.parquet"))

# prints:
# A: struct<key1: string, key2: string>
#   child 0, key1: string
#   child 1, key2: string
# B: list<element: int64>
#   child 0, element: int64
# C: binary
```

Parquet can handle these complex types using various means.

------------------------------------------------------------------------

## Parquet - Compression


```python
import os
import pyarrow as pa
from pyarrow import parquet

# create a pyarrow table
table = pa.Table.from_pydict(
    {
        "A": [1, 2, 3, 4, 5],
        "B": ["foo", "bar", "baz", "qux", "quux"],
        "C": [0.1, 0.2, 0.3, 0.4, 0.5],
    }
)

# Write Parquet file with Snappy compression
parquet.write_table(table=table, 
  where="example.snappy.parquet",
  compression="SNAPPY"
)
```

Parquet files may leverage compression to help reduce file size and increase data performance.

------------------------------------------------------------------------

## Parquet - Metadata as "first-class" object

<br>

![](https://mermaid.ink/img/pako:eNpNTstOxDAM_JXK56rqO22uWziBhIAThINpsjSiSUo2ESxV_510F9jaPtgzY3tm6A0XQGE_ms9-QOuix47pKMTBv75ZnIaIwR3aDy9cdC1HweBMr8GlFb2TRkc39xd0s7gzo1c66tDhdk9ofhmUcMiD4JnB7W_L4OXMn4RrQgxKWIWSB6_zSjJwg1DBDg0tR_u-PliCDr0zD0fdA3XWixj8FE6KTmIwpYDucTz8o1dcOmP_lBPqJ2PUZgQ6wxfQLC2SnKQ1KbO2qsqUtDEcA9xkSV62TaiqaQjJyRLD9-lCmtRpVpCqLUhZZ01VLD8oJG2d?type=png)

<br>

The Parquet format treats data about the data (metadata) distinctly from that of column value data ([link](https://parquet.apache.org/docs/file-format/metadata/)).

------------------------------------------------------------------------

## Parquet - Metadata as "first-class" object

```python
import pyarrow as pa
from pyarrow import parquet

# create a pyarrow table
table = pa.Table.from_pydict(
    {
        "A": [1, 2, 3],
        "B": ["foo", "bar", "baz"],
        "C": [0.1, 0.2, 0.3],
    }
)

# add custom metadata to table
table = table.replace_schema_metadata(metadata={"data-producer": "CU DBMI SET"})

# write the pyarrow table to a parquet file
parquet.write_table(table=table, where="example.snappy.parquet", compression="SNAPPY")

# read the schema
print(parquet.read_schema(where="example.snappy.parquet"))

# prints
# A: int64
# B: string
# C: double
# -- schema metadata --
# data-producer: 'CU DBMI SET'
```

We can customize metadata inside of a Parquet file so that the files always include certain information.

------------------------------------------------------------------------

## Parquet - Multi-file "datasets"

<br>

```text
parquet_datasets/
‚îú‚îÄ‚îÄ dataset0/
‚îÇ   ‚îú‚îÄ‚îÄ part-0.parquet
‚îÇ   ‚îú‚îÄ‚îÄ part-1.parquet
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îú‚îÄ‚îÄ dataset1/
‚îÇ   ‚îú‚îÄ‚îÄ part-0.parquet
‚îÇ   ‚îú‚îÄ‚îÄ part-1.parquet
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îî‚îÄ‚îÄ ...
```

<br>

Parquet files may be used individually or treated as a "[dataset](https://arrow.apache.org/docs/python/api/dataset.html)" through file groups which include the same schema (column names and types).

------------------------------------------------------------------------

## Parquet - Multi-file "datasets"

```python
import pathlib
import pyarrow as pa
from pyarrow import parquet

pathlib.Path("./dataset").mkdir(exist_ok=True)

# create pyarrow tables
table_1 = pa.Table.from_pydict({"A": [1]})
table_2 = pa.Table.from_pydict({"A": [2, 3]})

# write the pyarrow table to parquet files
parquet.write_table(table=table_1, where="./dataset/example_1.parquet")
parquet.write_table(table=table_2, where="./dataset/example_2.parquet")

# read the parquet dataset
print(parquet.ParquetDataset("./dataset").read())

# prints (note that, for ex., [1] is a row group of column A)
# pyarrow.Table
# A: int64
# ----
# A: [[1],[2,3]]
```

Parquet datasets may be read using directory paths (or lists of individual files).

------------------------------------------------------------------------

## Parquet - Arrow cross-lingual interchange


![](assets/2024-03-28-10-40-41.png)


The Parquet format has robust support and integration with the [Apache Arrow](https://arrow.apache.org/docs/index.html) memory format, enabling consistent multi-lingual interchange.

------------------------------------------------------------------------

## Parquet - Arrow cross-lingual interchange

```python
import pathlib
import pyarrow as pa
from pyarrow import parquet

# create a pyarrow table
table = pa.Table.from_pydict(
    {
        "A": [1, 2, 3],
        "B": ["foo", "bar", "baz"],
        "C": [0.1, 0.2, 0.3],
    }
)

# write the pyarrow table to a parquet file
parquet.write_table(table=table, where="example.parquet")

# show schema of table and parquet file
print(table.schema.types)
print(parquet.read_schema("example.parquet").types)

# prints
# [DataType(int64), DataType(string), DataType(double)]
# [DataType(int64), DataType(string), DataType(double)]
```

We can consistently read from Parquet files using Arrow to unify how in-memory data are treated.

------------------------------------------------------------------------

## Parquet - Arrow cross-lingual interchange

<br>

![](assets/2024-03-28-11-02-05.png)

- Cross-lingual implementation means we have more people participating in the same "data conversations".
- It also [decouples](https://en.wikipedia.org/wiki/Coupling_(computer_programming)) us from the limitations of "one language".

------------------------------------------------------------------------

## Parquet - FAIR data

![](assets/2024-03-28-11-04-36.png)

- These same principles can help enable FAIR data practices.

<span style="font-size:.7em;font-style:italic;">(Image: ["FAIR guiding principles for data resources" by SangyaPundir, 2016, Wikimedia Commons, CC BY-SA 4.0 DEED](https://commons.wikimedia.org/wiki/File:FAIR_data_principles.jpg))</span>


------------------------------------------------------------------------

## How can you use Parquet?

<br>

Below are a list of just a few popular places where you can use Parquet.

- __Python__
  - Pandas ([`pd.DataFrame.to_parquet()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_parquet.html), [`pd.read_parquet()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_parquet.html))
  - Apache Spark ([Spark SQL Guide: Parquet Files](https://spark.apache.org/docs/latest/sql-data-sources-parquet.html))
  - PyTorch ([`ParquetDataFrameLoader`](https://pytorch.org/data/main/generated/torchdata.datapipes.iter.ParquetDataFrameLoader.html))
  - PyArrow ([PyArrow: Reading and Writing the Apache Parquet Format](https://arrow.apache.org/docs/python/parquet.html))

------------------------------------------------------------------------

## How can you use Parquet?

<br>

Below are a list of just a few popular places where you can use Parquet.

- __R__
  - dplyr ([Arrow: Working with Arrow Datasets and dplyr](https://arrow.apache.org/docs/2.0/r/articles/dataset.html))
  - Arrow ([`write_parquet()`](https://arrow.apache.org/docs/r/reference/write_parquet.html), [`read_parquet()`](https://arrow.apache.org/docs/r/reference/read_parquet.html))
  - DuckDB ([DuckDB R API](https://duckdb.org/docs/api/r.html), [DuckDB: Reading and Writing Parquet Files](https://duckdb.org/docs/data/parquet/overview))


------------------------------------------------------------------------

## You might also be interested in...

<br><br>

- __Apache Arrow__ - an in-memory format which enables high-performance and unified memory handling across multiple languages. ([Apache Arrow documentation](https://arrow.apache.org/))
- __DuckDB__ - which enables SQL queries over local or remote Parquet files. ([DuckDB documentation](https://duckdb.org/docs/data/parquet/overview), [CU-DBMI blog post](https://cu-dbmi.github.io/set-website/2022/12/05/Data-Engineering-with-SQL-Arrow-and-DuckDB.html))
- __Ibis__ - Pythonic API for Parquet (and many other format) data handling ([Ibis documentation](https://ibis-project.org/how-to/input-output/duckdb-parquet))


------------------------------------------------------------------------

## Thank you!

<br>
<br>

Thank you for attending! Questions / comments?

<br>

Please don't hesitate to reach out!

<br>

- <img width="40" height="40" style="margin: 0;" src="https://cdn.simpleicons.org/htmx/black"> [CU Anschutz DBMI SET Team](https://cu-dbmi.github.io/set-website/about/#the-team)
- <img width="40" height="40" style="margin: 0;" src="https://cdn.simpleicons.org/github/black"> [@d33bs](https://github.com/d33bs)

